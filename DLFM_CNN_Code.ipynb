{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "988d443b",
   "metadata": {},
   "source": [
    "# Deep Learning at the Frontier Market Comparing CNN and Classical Machine Learning for Stock Prediction on the Nairobi Securities Exchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7783b04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all relevant libraries\n",
    "from collections import deque\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import os\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "import gc\n",
    "import psutil\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5950c8ba",
   "metadata": {},
   "source": [
    "** Viewing the raw file first few and last few entries for validity check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aec0fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick peek into raw file to understand header rows\n",
    "with open(\"NSE_merged_cleaned_2007_2020.csv\") as f:\n",
    "    for _ in range(10):\n",
    "        print(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df71231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick peek into raw file to understand tai rows\n",
    "with open(\"NSE_merged_cleaned_2007_2020.csv\") as f:\n",
    "    last_lines = deque(f, maxlen=10)\n",
    "\n",
    "for line in last_lines:\n",
    "    print(line, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24fc062",
   "metadata": {},
   "source": [
    "### Section 3.1: Data Collection and Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c21aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"NSE_merged_cleaned_2007_2020.csv\")\n",
    "df = df.drop(index=0).reset_index(drop=True)\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "\n",
    "df['DATE'] = pd.to_datetime(df['DATE'], errors='coerce', dayfirst=False)\n",
    "numeric_cols = ['12m Low', '12m High', 'Day Low', 'Day High',\n",
    "                'Day Price', 'Previous', 'Change', 'Change%', 'Volume', 'Adjust']\n",
    "\n",
    "\n",
    "df[numeric_cols] = df[numeric_cols].replace({',': '', '-': np.nan, '‚Äì': np.nan}, regex=True)\n",
    "df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "df = df.dropna(subset=['DATE', 'CODE', 'Day Price'])\n",
    "df = df.sort_values(['CODE', 'DATE']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(df['DATE'].min(), df['DATE'].max())\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975b5ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check = pd.read_csv(\"NSE_merged_cleaned_2007_2020.csv\")\n",
    "df_check = df_check.drop(index=0).reset_index(drop=True)\n",
    "df_check.columns = df_check.columns.str.strip()\n",
    "\n",
    "\n",
    "bad_dates = df_check[~pd.to_datetime(df_check['DATE'], errors='coerce', dayfirst=False).notna()]\n",
    "print(\"Number of rows with bad DATEs:\", bad_dates.shape[0])\n",
    "print(\"Sample bad DATE values:\")\n",
    "print(bad_dates['DATE'].unique()[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e17c8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"NSE_merged_cleaned_2007_2020.csv\")\n",
    "df = df.drop(index=0).reset_index(drop=True)\n",
    "df.columns = df.columns.str.strip()\n",
    "df['DATE'] = df['DATE'].astype(str).str.strip()\n",
    "df = df[~df['DATE'].str.lower().isin(['date'])]\n",
    "\n",
    "\n",
    "def try_parse(date_str):\n",
    "    for fmt in (\"%m/%d/%Y\", \"%d-%b-%y\"):\n",
    "        try:\n",
    "            return pd.to_datetime(date_str, format=fmt)\n",
    "        except:\n",
    "            continue\n",
    "    return pd.NaT\n",
    "\n",
    "df['DATE'] = df['DATE'].apply(try_parse)\n",
    "\n",
    "\n",
    "numeric_cols = ['12m Low', '12m High', 'Day Low', 'Day High',\n",
    "                'Day Price', 'Previous', 'Change', 'Change%', 'Volume', 'Adjust']\n",
    "df[numeric_cols] = df[numeric_cols].replace({',': '', '-': np.nan, '‚Äì': np.nan}, regex=True)\n",
    "df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "\n",
    "df = df.dropna(subset=['DATE', 'CODE', 'Day Price'])\n",
    "df = df.sort_values(['CODE', 'DATE']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"Date range:\", df['DATE'].min(), df['DATE'].max())\n",
    "print(\"Total rows:\", df.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad810693",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726c122d",
   "metadata": {},
   "source": [
    "### Section 3.2: Exploratory Data Analysis (Optional Plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1923a72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['return_1d'] = df.groupby('CODE')['Day Price'].pct_change()\n",
    "df['return_1d_clipped'] = df['return_1d'].clip(-0.1, 0.1)\n",
    "df['spread_pct'] = (df['Day High'] - df['Day Low']) / df['Day Price']\n",
    "df['price_up'] = df.groupby('CODE')['Day Price'].diff() > 0\n",
    "df['vol_5d'] = df.groupby('CODE')['return_1d'].rolling(5).std().reset_index(level=0, drop=True)\n",
    "df['year'] = df['DATE'].dt.year\n",
    "\n",
    "\n",
    "print(\"Skewness:\", df['return_1d'].skew())\n",
    "print(\"Kurtosis:\", df['return_1d'].kurt())\n",
    "print(\"Date Range:\", df['DATE'].min(), df['DATE'].max())\n",
    "\n",
    "\n",
    "df['return_1d'].hist(bins=100, range=(-0.1, 0.1), density=True, alpha=0.6)\n",
    "plt.axvline(df['return_1d'].mean(), color='red', linestyle='--', label='Mean')\n",
    "plt.title(\"Return Distribution\")\n",
    "plt.xlabel(\"1-Day Return\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525fe368",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_traded = df.groupby('CODE')['Volume'].mean().sort_values(ascending=False).head(10)\n",
    "most_traded.plot(kind='bar', title='Top 10 Most Traded Stocks by Volume')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950b18ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "scom = df[df['CODE'] == 'SCOM'].copy()\n",
    "scom['30D MA'] = scom['Day Price'].rolling(30).mean()\n",
    "plt.plot(scom['DATE'], scom['Day Price'], alpha=0.5, label='Price')\n",
    "plt.plot(scom['DATE'], scom['30D MA'], color='red', label='30D MA')\n",
    "plt.title(\"Safaricom Price and 30-Day MA\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb37249",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_returns = df.groupby('CODE')['return_1d'].mean().sort_values()\n",
    "mean_returns.head(10).plot(kind='bar', color='crimson', title='Bottom 10 Stocks by Avg Daily Return')\n",
    "plt.tight_layout(); plt.grid(True); plt.show()\n",
    "mean_returns.tail(10).plot(kind='bar', color='green', title='Top 10 Stocks by Avg Daily Return')\n",
    "plt.tight_layout(); plt.grid(True); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a584c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spread_avg = df.groupby('CODE')['spread_pct'].mean().sort_values(ascending=False).head(10)\n",
    "spread_avg.plot(kind='bar', title='Top 10 Stocks by Avg Intraday Spread', color='purple')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66d98cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "top3 = most_traded.head(3).index.tolist()\n",
    "plt.figure(figsize=(12, 5))\n",
    "for code in top3:\n",
    "    tmp = df[df['CODE'] == code]\n",
    "    plt.plot(tmp['DATE'], tmp['Volume'].rolling(20).mean(), label=code)\n",
    "plt.title(\"20-Day Rolling Volume (Top 3 Stocks)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28337d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "up_ratio = df.groupby('CODE')['price_up'].mean().sort_values(ascending=False).head(10)\n",
    "up_ratio.plot(kind='bar', color='green', title='Top 10 by Upward Price Ratio')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d731b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_stock = df['CODE'].value_counts().idxmax()\n",
    "vol_data = df[df['CODE'] == top_stock].copy()\n",
    "vol_data['rolling_vol'] = vol_data['return_1d'].rolling(10).std()\n",
    "plt.plot(vol_data['DATE'], vol_data['rolling_vol'])\n",
    "plt.title(f\"10D Rolling Volatility: {top_stock}\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0efc3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spread_clusters = df.groupby('CODE')['spread_pct'].mean()\n",
    "spread_clusters.plot(kind='hist', bins=30, edgecolor='black', title=\"Avg Spread Distribution\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2353a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_table = df.groupby(['CODE', 'year']).size().unstack(fill_value=0)\n",
    "display(activity_table.head(10))  # Show sample of stock activity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b3734b",
   "metadata": {},
   "source": [
    "### Section 3.3: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982dcf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_normalize = ['Day Price', 'Day High', 'Day Low', 'Previous', 'Volume']\n",
    "df_scaled = df.copy() \n",
    "\n",
    "\n",
    "for code in df_scaled['CODE'].unique():\n",
    "    mask = df_scaled['CODE'] == code\n",
    "    subset = df_scaled.loc[mask, features_to_normalize]\n",
    "    if subset.dropna().shape[0] > 1 and not (subset.nunique() == 1).any():\n",
    "        scaler = StandardScaler()\n",
    "        df_scaled.loc[mask, features_to_normalize] = scaler.fit_transform(subset)\n",
    "    else:\n",
    "        df_scaled.loc[mask, features_to_normalize] = np.nan\n",
    "\n",
    "\n",
    "def add_features(group):\n",
    "    group = group.sort_values(\"DATE\").copy()\n",
    "    group[\"momentum_5d\"] = group[\"Day Price\"].pct_change(periods=5, fill_method=None)\n",
    "    group[\"ma_5\"] = group[\"Day Price\"].rolling(window=5).mean()\n",
    "    group[\"ma_10\"] = group[\"Day Price\"].rolling(window=10).mean()\n",
    "    group[\"ma_diff\"] = group[\"ma_5\"] - group[\"ma_10\"]\n",
    "    group[\"volatility_10d\"] = group[\"return_1d_clipped\"].rolling(window=10).std()\n",
    "    group[\"hl_spread\"] = (group[\"Day High\"] - group[\"Day Low\"]) / group[\"Day Price\"]\n",
    "    group[\"vol_zscore\"] = (\n",
    "        (group[\"Volume\"] - group[\"Volume\"].rolling(window=10).mean()) /\n",
    "        group[\"Volume\"].rolling(window=10).std()\n",
    "    )\n",
    "    return group\n",
    "\n",
    "\n",
    "df_features_list = []\n",
    "for code, group in df_scaled.groupby(\"CODE\"):\n",
    "    features = add_features(group)\n",
    "    df_features_list.append(features)\n",
    "\n",
    "df_features = pd.concat(df_features_list, axis=0)\n",
    "\n",
    "\n",
    "df_features['ma_20'] = df_features.groupby(\"CODE\")['Day Price'].transform(lambda x: x.rolling(window=20).mean())\n",
    "\n",
    "\n",
    "df_features = df_features.dropna(subset=[\n",
    "    \"momentum_5d\", \"ma_5\", \"ma_10\", \"ma_diff\",\n",
    "    \"volatility_10d\", \"hl_spread\", \"vol_zscore\", \"ma_20\"\n",
    "])\n",
    "\n",
    "\n",
    "print(df_features[['CODE', 'DATE', 'Day Price', 'momentum_5d', 'ma_diff', 'volatility_10d']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41795f9",
   "metadata": {},
   "source": [
    "### Section 3.4: Chart-Based Image Construction for CNN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c98079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "process = psutil.Process(os.getpid())\n",
    "print(\"üìå Starting final image generation with OHLC style...\")\n",
    "\n",
    "\n",
    "output_dir = \"cnn_final_OHLC_BW\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "window = 20\n",
    "label_horizon = 5\n",
    "img_size = 32\n",
    "chart_height = 18\n",
    "volume_start = 20\n",
    "label_data = []\n",
    "\n",
    "\n",
    "csv_path = \"NSE_merged_cleaned_2007_2020.csv\"\n",
    "df = pd.read_csv(csv_path, skiprows=[1])\n",
    "df.columns = df.columns.str.strip()\n",
    "df['DATE'] = pd.to_datetime(df['DATE'], errors='coerce')\n",
    "\n",
    "numeric_cols = ['12m Low', '12m High', 'Day Low', 'Day High',\n",
    "                'Day Price', 'Previous', 'Change', 'Change%', 'Volume', 'Adjust']\n",
    "df[numeric_cols] = df[numeric_cols].replace(',', '', regex=True).apply(pd.to_numeric, errors='coerce')\n",
    "df = df.dropna(subset=['DATE', 'CODE', 'Day Price', 'Day High', 'Day Low', 'Volume'])\n",
    "df = df.sort_values(['CODE', 'DATE']).reset_index(drop=True)\n",
    "df['ma_20'] = df.groupby(\"CODE\")['Day Price'].transform(lambda x: x.rolling(window=20).mean())\n",
    "df = df.dropna(subset=['ma_20'])\n",
    "\n",
    "\n",
    "for code in df['CODE'].unique():\n",
    "    stock = df[df['CODE'] == code].reset_index(drop=True)\n",
    "    if len(stock) < window + label_horizon:\n",
    "        continue\n",
    "\n",
    "    for i in range(len(stock) - window - label_horizon):\n",
    "        snippet = stock.iloc[i:i + window]\n",
    "        if snippet[['Day Price', 'Day High', 'Day Low', 'Volume', 'ma_20']].isnull().any().any():\n",
    "            continue\n",
    "\n",
    "        price_cols = ['Day Price', 'ma_20', 'Day High', 'Day Low']\n",
    "        price_min = snippet[price_cols].min().min()\n",
    "        price_max = snippet[price_cols].max().max()\n",
    "        vol_max = snippet['Volume'].max()\n",
    "\n",
    "        def scale(series, out_min, out_max):\n",
    "            return ((series - price_min) / (price_max - price_min + 1e-6) *\n",
    "                    (out_max - out_min) + out_min).clip(out_min, out_max)\n",
    "\n",
    "        close_scaled = scale(snippet['Day Price'], 0, chart_height)\n",
    "        ma_scaled = scale(snippet['ma_20'], 0, chart_height - 2)\n",
    "        high_scaled = scale(snippet['Day High'], 0, chart_height)\n",
    "        low_scaled = scale(snippet['Day Low'], 0, chart_height)\n",
    "        volume_scaled = (snippet['Volume'] / (vol_max + 1e-6) *\n",
    "                         (img_size - volume_start - 1)).clip(0, img_size - volume_start - 1)\n",
    "\n",
    "        img = Image.new(\"1\", (img_size, img_size), 0)\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        x_step = img_size / window\n",
    "\n",
    "        \n",
    "        for j in range(window):\n",
    "            x = int(j * x_step)\n",
    "            high_y = max(0, int(chart_height - high_scaled.iloc[j]))\n",
    "            low_y = min(chart_height, int(chart_height - low_scaled.iloc[j]))\n",
    "            close_y = min(chart_height, max(0, int(chart_height - close_scaled.iloc[j])))\n",
    "\n",
    "            \n",
    "            draw.line([(x, high_y), (x, low_y)], fill=1)\n",
    "            \n",
    "            draw.line([(x - 1, close_y), (x + 1, close_y)], fill=1)\n",
    "\n",
    "        \n",
    "        for j in range(1, window):\n",
    "            x0 = int((j - 1) * x_step)\n",
    "            x1 = int(j * x_step)\n",
    "            y0 = int(chart_height - ma_scaled.iloc[j - 1])\n",
    "            y1 = int(chart_height - ma_scaled.iloc[j])\n",
    "            steps = max(abs(x1 - x0), abs(y1 - y0))\n",
    "            for s in range(0, steps, 2):  \n",
    "                xi = int(x0 + s * (x1 - x0) / steps)\n",
    "                yi = int(y0 + s * (y1 - y0) / steps)\n",
    "                if 0 <= xi < img_size and 0 <= yi < volume_start:\n",
    "                    img.putpixel((xi, yi), 1)\n",
    "\n",
    "        \n",
    "        for j in range(window):\n",
    "            x = int(j * x_step)\n",
    "            v_height = int(volume_scaled.iloc[j])\n",
    "            vol_top = img_size - 1\n",
    "            vol_bottom = img_size - 1 - v_height\n",
    "            draw.line([(x, vol_bottom), (x, vol_top)], fill=1)\n",
    "\n",
    "        \n",
    "        current_price = stock.iloc[i + window]['Day Price']\n",
    "        future_price = stock.iloc[i + window + label_horizon - 1]['Day Price']\n",
    "        label = int(future_price > current_price)\n",
    "\n",
    "        date_str = snippet['DATE'].iloc[-1].strftime('%Y%m%d')\n",
    "        fname = f\"{code}_{date_str}.png\"\n",
    "        img.save(os.path.join(output_dir, fname))\n",
    "        label_data.append((fname, label))\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "labels_df = pd.DataFrame(label_data, columns=[\"filename\", \"label\"])\n",
    "labels_df.to_csv(os.path.join(output_dir, \"labels.csv\"), index=False)\n",
    "\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "ram_used = process.memory_info().rss / 1024 / 1024\n",
    "print(f\"\\n‚úÖ Final image generation complete.\")\n",
    "print(f\"‚è±Ô∏è Runtime: {elapsed:.2f} seconds\")\n",
    "print(f\"üß† RAM Used: {ram_used:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd386e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = \"cnn_final_OHLC_BW\"\n",
    "labels_path = os.path.join(img_dir, \"labels.csv\")\n",
    "\n",
    "\n",
    "labels_df = pd.read_csv(labels_path)\n",
    "\n",
    "\n",
    "labels_df[\"exists\"] = labels_df[\"filename\"].apply(lambda x: os.path.exists(os.path.join(img_dir, x)))\n",
    "\n",
    "\n",
    "total = len(labels_df)\n",
    "missing = (~labels_df[\"exists\"]).sum()\n",
    "print(f\"‚úÖ Total samples in labels.csv: {total}\")\n",
    "print(f\"‚ö†Ô∏è Missing image files: {missing}\")\n",
    "print(\"\\nLabel Distribution:\")\n",
    "print(labels_df[\"label\"].value_counts())\n",
    "\n",
    "\n",
    "missing_files = labels_df[~labels_df[\"exists\"]]\n",
    "if not missing_files.empty:\n",
    "    print(\"\\nMissing Files Preview:\")\n",
    "    print(missing_files.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9507f06",
   "metadata": {},
   "source": [
    "\n",
    "### Section 3.5: CNN Model Training & Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0eb62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "process = psutil.Process(os.getpid())\n",
    "\n",
    "\n",
    "label_path = \"cnn_bw_images_v2/labels.csv\"\n",
    "label_df = pd.read_csv(label_path)\n",
    "\n",
    "\n",
    "def load_bw_image(filepath, size=(32, 32)):\n",
    "    img = Image.open(filepath).convert('L').resize(size)\n",
    "    arr = np.array(img)\n",
    "    arr = np.where(arr == 255, 1, 0)  \n",
    "    return np.expand_dims(arr, axis=-1)\n",
    "\n",
    "\n",
    "label_df = label_df[label_df['filename'].apply(lambda f: os.path.exists(f))]\n",
    "\n",
    "\n",
    "X = np.array([load_bw_image(f) for f in label_df['filename']])\n",
    "y = label_df['label'].values\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(32, 32, 1)),\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "y_pred_prob = model.predict(X_test).ravel()\n",
    "y_pred_binary = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "print(\"‚úÖ Test Accuracy:\", test_acc)\n",
    "print(\"‚úÖ ROC AUC:\", roc_auc_score(y_test, y_pred_prob))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_binary))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_binary))\n",
    "\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title(\"CNN Training Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "ram_used = process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è Total runtime: {elapsed_time:.2f} seconds\")\n",
    "print(f\"üß† RAM used: {ram_used:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f04cfc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58521274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7eb59c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c3d70e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7aa7cad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a38a819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f30be1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a893ccd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea11565",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22e3c67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
